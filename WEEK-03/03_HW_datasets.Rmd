---
title: "Week 3 Assignment"
author: "Michaela Bayerlova"
date: "`r format(Sys.time(), '%d %B %Y')`" 
output: 
  pdf_document: 
    citation_package: natbib
    latex_engine: pdflatex
    template: ./../latex-templates/homework.tex
    fig_caption: true
    number_section: true
  html_document:
    df_print: paged
course: "STATS 419 Analysis of Multivariate Analysis"
course-short: STATS419
instructor: Monte J. Shaffer
email: michaela.bayerlova@wsu.edu
wsu_id: NA

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE)
```



```{r files, message=FALSE}
library(devtools); # required for function source_url to work
github.path = "https://raw.githubusercontent.com/MichaelaB1/WSU_STATS419_FALL2020/";
source_url(paste0(github.path, "master/functions/libraries.R"));
           
source_url(paste0(github.path, "master/functions/functions-imdb.R"));

```

# Matrix
Create the "rotate matrix" functions described in lecture. Apply to the example "myMatrix".
```{r, mychunk-matrix-include, message=FALSE}
source_url(paste0(github.path, "master/functions/functions-matrix.R"));

myMatrix = matrix( c (
			                1, 0, 2,
			                0, 3, 0,
			                4, 0, 5
				               ), nrow=3, byrow=T);

transposeMatrix = function(mat)
{
  t(mat);	
          }
```


```{r, mychunk-matrix}
#my.url = paste0(github.path, "master/WEEK-03/functions/HW2_functions.R");
#my.url;
source_url(paste0(github.path, "master/WEEK-03/functions/HW2_functions.R"));
# Rotate clockwise
rotateMatrix90_c(myMatrix);
rotateMatrix180_c(myMatrix);
rotateMatrix270_c(myMatrix);
# Rotate counter clockwise
rotateMatrix90_cc(myMatrix);
rotateMatrix180_cc(myMatrix);
rotateMatrix270_cc(myMatrix);
```



# IRIS Scatterplot
Recreate the graphic for the IRIS Data Set using R. Same titles, same scales, same colors. See: https://en.wikipedia.org/wiki/Iris_flower_data_set#/media/File:Iris_dataset_scatterplot.svg
```{r, mychunk-iris-include, fig.cap="Iris Data scatterplot"}
# Iris Data Set
IRIS_Data = iris 
class(iris)
# Scatterplot
pairs(IRIS_Data[1:4], main = "Iris Data (red=setosa,green=versicolor,blue=virginica)", 
      pch = 21, bg = c("red", "green", "blue")[unclass(IRIS_Data$Species)])

```

# IRIS Question
Right 2-3 sentences concisely defining the IRIS Data Set.  Maybe search KAGGLE for a nice template.  
Be certain the final writeup are your own sentences (make certain you modify what you find, make it your own, but also cite where you got your ideas from). NOTE:  Watch the video, Figure 8 has a +5 EASTER EGG.

The Iris flower data set is a multivariate data set. The data set contains four measurements (sepals length and width, petals length and width) for 150 records of flowers. Each is represented in the three species of iris: Iris setosa, Iris versicolor and Iris virginica. The Iris setosa is from a wide range across the Arctic sea. The Iris versicolor is found in North America, like Eastern United States and Eastern Canada. The Iris virginica is native to eastern North America.



# Personality
Import "personality-raw.txt" into R.  Remove the V00 column.  Create two new columns from the current column "date_test":  year and week. Stack Overflow may help:  https://stackoverflow.com/questions/22439540/how-to-get-week-numbers-from-dates ... Sort the new data frame by YEAR, WEEK so the newest tests are first ... The newest tests (e.g., 2020 or 2019) are at the top of the data frame.  Then remove duplicates using the unique function based on the column "md5_email".  Save the data frame in the same "pipe-delimited format" ( | is a pipe ) with the headers.  You will keep the new data frame as "personality-clean.txt" for future work (you will not upload it at this time).  In the homework, for this tasks, report how many records your raw dataset had and how many records your clean dataset has.
```{r, mychunk-personality}
personality.folder = "C:/Users/michaela.bayerlova/Documents/STATS 419/reading assignments/";
raw.file = paste0(personality.folder, "personality-raw.txt");

#my_data = read.delim("C:\\Users\\michaela.bayerlova\\Documents\\STATS 419\\reading assignments\\personality-raw.txt", sep = "|", header = TRUE);

my_data = utils::read.csv(raw.file, header=TRUE, sep="|");

#head(my_data);
nrow_raw_data <- nrow(my_data);
dim(my_data);

# remove column V00
my_data$V00 <- NULL;

# Create two new columns from the current column "date_test":  year and week
d = strptime(my_data$date_test, format="%m/%d/%Y %H:%M");
d.year = as.numeric(strftime(d, format='%Y'));
d.week = as.numeric(strftime(d, format='%W'));
my_data$year = d.year;
my_data$week = d.week;
date_index = which(names(my_data)=="date_test");

# Remove date_test
my_data$date_test <- NULL;

# Sort the new data frame by YEAR, WEEK so the newest tests are first 
# my_data <- arrange(my_data, c("year","week"),);
df.dim = dim(my_data); 
df.pos = df.dim[2]; 
which( names(my_data)=="year" );
which( names(my_data)=="week" );
my_data = my_data[, c(1,62,63, 2:61)];

if(date_index == 1)
{
  reorder = c( (df.pos-1):df.pos, 2:(df.pos-2) );
}else{
     reorder = c( 1:(date_index-1), (df.pos-1):df.pos,  2:(df.pos-2) );
     }

my_data = my_data[, reorder];
my_data_sorted = my_data[ order(-my_data[,date_index],-my_data[,(1+date_index)]), ];
#head(my_data_sorted[,1:5]);

# Remove duplicates using the unique function based on the column "md5_email", only leave most recent
#my_data <- my_data[!duplicated(my_data$md5_email),];
u = unique( my_data_sorted["md5_email"] );
my_data_clean = my_data_sorted[!duplicated(my_data_sorted["md5_email"]), ];

# In the homework, for this tasks, report how many records your raw dataset had and
# how many records your clean dataset has
nrow_clean_data <- nrow(my_data_clean)
dim(my_data_clean);
#install.packages("data.table")
#library(data.table);
#fwrite(my_data_clean,"personality-clean.txt", sep="|",col.names = TRUE, row.names = FALSE);

clean.file = paste0(personality.folder, "personality-clean.txt");


utils::write.table(my_data_clean, file=clean.file, quote=FALSE, col.names=TRUE, row.names=FALSE, sep="|");

  
#utils::read.csv(mycache, header=TRUE, sep="|");


```



# Variance and Z-scores 
Write functions for doSummary and sampleVariance and doMode ... test these functions in your homework on the "monte.shaffer@gmail.com" record from the clean dataset.  Report your findings.  For this "monte.shaffer@gmail.com" record, also create z-scores.  Plot(x,y) where x is the raw scores for "monte.shaffer@gmail.com" and y is the z-scores from those raw scores.  Include the plot in your assignment, and write 2 sentences describing what pattern you are seeing and why this pattern is present.
```{r,mychunk-variance-intro}

library(humanVerseWSU);

x.norm = rnorm(100,0,1);
s.norm = doStatsSummary ( x.norm );
str(s.norm);  # mode is pretty meaningless on this data

x.unif = runif(100,0,1);
s.unif = doStatsSummary ( x.unif );
str(s.unif);  # mode is pretty meaningless on this data



#################################################################

doStatsSummary = function(x)
	{
	result = list();
		result$length = length(x);
	xx = stats::na.omit(x);
		result$length.na = length(x) - length(xx);
		result$length.good = length(xx);
	result$mean = mean(xx);
	result$mean.trim.05 = mean(xx, trim=0.05);
	result$mean.trim.20 = mean(xx, trim=0.20);

	result$median = stats::median(xx);
	result$MAD = stats::mad(xx);
	result$IQR = stats::IQR(xx);
	result$quartiles = stats::quantile(xx, prob=c(.25,.5,.75));
	result$deciles = stats::quantile(xx, prob=seq(0.1,0.9,by=0.1) );
	result$centiles = stats::quantile(xx, prob=seq(0.01,0.99,by=0.01) );

	result$median.weighted = matrixStats::weightedMad(xx);
	result$MAD.weighted = matrixStats::weightedMedian(xx);

	result$max = max(xx);
	result$min = min(xx);
	result$range = result$max - result$min;
	result$xlim = range(xx);

	result$max.idx = whichMax(x);
	result$min.idx = whichMin(x);

	result$mode = result$freq.max = doMode(x);  # elements with highest frequency
	result$which.min.freq = doModeOpposite(x);

	result$ylim = c( freqMin(xx), freqMax(xx) );

	# you could later get indexes of each mode(freq.max)/freq.min
	#using findAllIndexesWithValueInVector

	result$sd = stats::sd(xx);
	result$var = stats::var(xx);

	result$var.naive = doSampleVariance(x,"naive");
	result$var.2step = doSampleVariance(x,"2step");


	## normality
	result$shapiro = stats::shapiro.test(xx);
	result$shapiro.is.normal = list("0.10" = isTRUE(result$shapiro$p.value > 0.10), "0.05" = isTRUE(result$shapiro$p.value > 0.05), "0.01" = isTRUE(result$shapiro$p.value > 0.01) );

	result$outliers.z = findOutliersUsingZscores(x);
	result$outliers.IQR = findOutliersUsingIQR(x);

	#result$z = calculateZscores(x);

	result;
  }

#################################################################

doSampleVariance = function(x, method="two-pass")
	{
	x = stats::na.omit(x);
	if(method=="naive")
		{
		n = 0;
		sum = 0;
		sum2 = 0;

		for(i in 1:length(x))  ## stats::na.omit(x)
			{
			n = n + 1;
			sum = sum + x[i];
			sum2 = sum2 + x[i]*x[i];
			}

		if(n < 2) { return(NULL);} #
			x.bar = sum/n;
			s.var = (sum2 - (sum*sum)/n)/(n-1);

		} else	{
				# two-pass algorithm # testing
				n = sum = sum2 = 0;
				## first pass
				for(i in 1:length(x))  ## stats::na.omit(x)
					{
					n = n + 1;
					sum = sum + x[i];
					}
		if(n < 2) { return(NULL);} #
				x.bar = sum/n;
				## second pass
				for(i in 1:length(x))  ## stats::na.omit(x)
					{
					deviation = x[i] - x.bar;
					sum2 = sum2 + deviation * deviation;
					}
				s.var = sum2/(n-1);
				}

		s.sd = sqrt(s.var);
	list("x.bar"=x.bar,"s.var"=s.var,"s.sd"=s.sd);
	}

#################################################################

doMode = function(x) # alias ?
	{
	whichMaxFreq(x);
  }


whichMaxFreq = function(x)  # doMode
	{
	x.table = as.data.frame( table(x) );
		freq.max = max( x.table$Freq );
	x.list = x.table[x.table$Freq==freq.max,];
	xs = as.numeric( as.vector (x.list$x) );
	xs;
	}

# R does not have a "mode" function built in that will capture ties.
#   This function will.
# In the process, I wrote other functions that are also not robust in R.
# For example ?which.max versus my function ?whichMax

which.max( c(87, presidents[1:30], 87) );
whichMax( c(87, presidents[1:30], 87) );

## a function can also be referenced using class::method notation

base::which.max( c(87, presidents[1:30], 87) );
#humanVerseWUS::whichMax( c(87, presidents[1:30], 87) );

# typos ... 
humanVerseWSU::whichMax( c(87, presidents[1:30], 87) );

## this will prevent confusion if functions have the same name (in different packages)

#################################################################

# not a requirement for your homework, but here is a function that will do it.
calculateZscores = function(x, x.bar=NULL, s.hat=NULL)
	{
  if(is.numeric(x.bar) && is.numeric(s.hat)) { return ((x - x.bar) / s.hat);}
  # maybe throw a warning if one is null, but not the other
  if( (is.null(x.bar) + is.null(s.hat)) == 1)
      {
      warning("Only one value was entered for x.bar / s.hat ... Computing these values instead.")
      }


	dsv = doSampleVariance(x);

	x.bar = dsv$x.bar;
	s.hat = dsv$s.sd;

	if(is.null(s.hat)) { return (NULL); }  # we take care of 
	#division by zero in our custom sampleVarianceFunction

	(x - x.bar) / s.hat;
	}

```

## Variance
### Naive

```{r,mychunk-variance-naive}
# https://raw.githubusercontent.com/MonteShaffer/humanVerseWSU/
#master/humanVerseWSU/R/functions-standardize.R

v.norm = doSampleVariance(x.norm, "naive");


# if x is really small
vsmall.df = as.data.frame( t(unlist(v.norm)) );

x.small = x.norm;
for(i in 1:20)
  {
  # every loop make it 1000 times smaller
  # notice I am looping over "i" but not using it.
  x.small = standardizeToFactor(x.small, 1/1000);
  v.small = doSampleVariance(x.small, "naive");
    v.row = t(unlist(v.small));
  vsmall.df = rbind(vsmall.df, v.row);
  }

vsmall.df;



# if x is really big
vlarge.df = as.data.frame( t(unlist(v.norm)) );

x.large = x.norm;
for(i in 1:20)
  {
  # every loop make it 1000 times larger
  # notice I am looping over "i" but not using it.
  x.large = standardizeToFactor(x.large, 1000);
  v.large = doSampleVariance(x.large, "naive");
    v.row = t(unlist(v.large));
  vlarge.df = rbind(vlarge.df, v.row);
  }

vlarge.df;


## from these two experiments it looks okay!

## CS purists say it will fail eventually 
## maybe I have to use a smaller n to demo failure?

## examine the function , the failure point is:    sum2 = sum2 + x[i]*x[i];
## [+5 Easter to first x-vec of 100 numbers that causes "naive" to fail!]
## by fail, I mean the "two-pass" approach and built ?sd or ?var function
## shows something entirely different ...

```


### Traditional Two Pass
```{r,mychunk-variance-2pass}

v2.norm = doSampleVariance(x.norm, "two-pass");
v2b.norm = doSampleVariance(x.norm);  # default value is "two-pass" in the function
v2c.norm = doSampleVariance(x.norm, "garblideljd=-gook"); # if logic defaults to "two-pass"

unlist(v2.norm);
unlist(v2b.norm);
unlist(v2c.norm);

```


## Z-scores
```{r,mychunk-apply-z-score}

# the built in function ?scale you should find useful.
# a z-score is taken on a vector of data requires x.bar and s.hat
# generally, we assume x.bar and s.hat comes from the vector of data. 
# It doesn't have to.

library(digest);
md5_monte = digest("monte.shaffer@gmail.com", algo="md5");  # no workee???
md5_monte = "b62c73cdaf59e0a13de495b84030734e";

# jQuery [b62c73cdaf59e0a13de495b84030734e]     https://www.jqueryscript.net/demo/MD5-Hash-String/
# Javascript [b62c73cdaf59e0a13de495b84030734e] http://md5.mshaffer.com/
# PHP     [b62c73cdaf59e0a13de495b84030734e]    https://onlinegdb.com/rJUGCTkrw
# Python enthusiasts:  I recommend WingIDE  ... https://wingware.com/
# [+5 investigate the issue... write a Python function that passes in a string
#   and returns a md5 string, write an onlinegdb.com example for C,
#   and write an onlinegdb.com example for C++ ... summarize your findings.]

row = my_data_clean[my_data_clean$md5_email == md5_monte, ];
vec.start = getIndexOfDataFrameColumns(row,"V01");  # 5
vec.end = getIndexOfDataFrameColumns(row,"V60"); # 64

vec = as.numeric( row[vec.start:vec.end] );  # vector functions require "vector form"
# recall the concept of a vector basis? (e.g., basis of vector space)
# linear combinations of this basis?


vdf = as.data.frame( t(vec) );  # dim(vdf) tells me to transpose it.
  myRows = c("raw");

z.vec = calculateZscores(vec);
  vdf = rbind(vdf,z.vec);     myRows=c(myRows,"z-scores");

z.vec30 = standardizeToFactor(vec, 30);
  vdf = rbind(vdf,z.vec30);  myRows=c(myRows,"30x");

z.vecmin = standardizeToMin(vec);  
# like z-scores, should rewrite to allow it be passed in, by default it computes
  vdf = rbind(vdf,z.vecmin);  myRows=c(myRows,"min");

z.vecmax = standardizeToMax(vec);  
# like z-scores, should rewrite to allow it be passed in, by default it computes
  vdf = rbind(vdf,z.vecmax);  myRows=c(myRows,"max");
  
z.vecN = standardizeToN(vec);  
# like z-scores, should rewrite to allow it be passed in, by default it computes
  vdf = rbind(vdf,z.vecN);  myRows=c(myRows,"N");

z.vecSum = standardizeToSum (vec);  
# like z-scores, should rewrite to allow it be passed in, by default it computes
  vdf = rbind(vdf,z.vecSum);  myRows=c(myRows,"Sum");

z.vecBound = standardizeFromOneRangeToAnother(vec, c(0,1) );  
# like z-scores, should rewrite to allow it be passed in, by default it computes
  vdf = rbind(vdf,z.vecBound);  myRows=c(myRows,"[0,1]");

rownames(vdf) = myRows;

tvdf = as.data.frame( t(vdf) ); # why transpose it?

graphics::plot( tvdf );

# linear transformations are "linear" ... should not be surprising
# how does perfect linearity relate to "correlation"?

# Multiplying by a negative number (not shown) is also a vector-basis manipulation.
# As is rotating by an angle.
# What is an example of a nonlinear combination? 
# Hint look at your will/denzel problem.
# plot(will$movies.50[,c(1,6,7:10)]); ... one relationship is strong, nonlinear


```





# Will vs. Denzel
Compare Will Smith and Denzel Washington. [See 03_n greater 1-v2.txt for the necessary functions and will-vs-denzel.txt for some sample code and in DROPBOX: `\__student_access__\unit_01_exploratory_data_analysis\week_02\imdb-example`]  You will have to create a new variable $millions.2000 that converts each movie's $millions based on the $year of the movie, so all dollars are in the same time frame.  You will need inflation data from about 1980-2020 to make this work.
`The dataset of Will Smith and Denzel Washington and the infrmation about movies they have participated in comes from the IMDB website, which can get updated and therefore is used very up to date.\citep{IMDB:2020}.`
```{r, mychunk-imdb_include, message=FALSE}
source_url(paste0(github.path,"master/functions/functions-imdb.R"));
```


## Will Smith
```{r, mychunk-will, fig.cap= c("Will Smith scatterplot: IMDB(2020)", "Will Smith boxplot raw millions: IMDB(2020)")}
nmid = "nm0000226";
 	will = grabFilmsForPerson(nmid);
 	plot(will$movies.50[,c(1,6,7:10)], ylim=c(0,200), xlim=c(0,200));
  	boxplot(will$movies.50$millions);
		widx =  which.max(will$movies.50$millions);
	will$movies.50[widx,];
		summary(will$movies.50$year);  

```

## Denzel Washington
```{r, mychunk-denzel, fig.cap= c("Denzel Washington scatterplot: IMDB(2020)", "Denzel Washington boxplot raw millions: IMDB(2020)")}
nmid = "nm0000243";
 	denzel = grabFilmsForPerson(nmid);
 	plot(denzel$movies.50[,c(1,6,7:10)], ylim=c(0,360), xlim=c(0,360));
  	boxplot(denzel$movies.50$millions);
		didx =  which.max(denzel$movies.50$millions);
	denzel$movies.50[didx,];
		summary(denzel$movies.50$year);
```

## Boxplot of Top-50 movies using Raw Dollars
```{r, mychunk-boxplot-raw, fig.cap= c("Will Smith vs Denzel Washington boxplot of Top-50 movies using Raw Millions: IMDB(2020)")}
par(mfrow=c(1,2));
	boxplot(will$movies.50$millions, main=will$name, ylim=c(0,360), ylab="Raw Millions" );
	boxplot(denzel$movies.50$millions, main=denzel$name, ylim=c(0,360), ylab="Raw Millions" );
	
	par(mfrow=c(1,1));

```

## Film Count Of Will Smith
```{r, mychunk-filmcount-will}
new.will = will$movies.50;
new.will$nmid = will$nmid;
new.will$name = will$name;
new.will$countfilms = will$countfilms$totalcount;
new.will = new.will[, c(12,13,14, 1:11)];
```

## Film Count Of Denzel Washington
```{r, mychunk-filmcount-denzel}
new.denzel = denzel$movies.50;
new.denzel$nmid = denzel$nmid;
new.denzel$name = denzel$name;
new.denzel$countfilms = denzel$countfilms$totalcount;
new.denzel = new.denzel[, c(12,13,14, 1:11)];
new.denzel$countfilms;
```

## Combined Dataframe of Will Smith and Denzel Washington
```{r, mychunk-will-denzel-df}
df.will.denzel = rbind(new.will, new.denzel);
```
This brief analysis can lead to a conclusion.
The box plots are showing, that Will Smith has a higher variety, like a bigger spread of earnings for the movies he shot than Denzel Washington. The mean is a little higher than 50 Millions, whereas the mean of Denzel Washington is on about 50 Million.
The observation of the number of films both actors played in gives, that Will Smith had 111 and Denzel Washington 61 movies. The result can lead to the conclusion that Will Smith is a better actor because he has been part of more movies and therefore has been asked more often to do so. 
Based on that data you could say that Will Smith is a better actor but in reality this is a very subjective question. Every person can answer it differently.


# Side by side comparison
Build side-by-side box plots on several of the variables (including #6) to compare the two movie stars.  After each box plot, write 2+ sentence describing what you are seeing, and what conclusions you can logically make.  You will need to review what the box plot is showing with the box portion, the divider in the box, and the whiskers.

## Adjusted Dollars (2000)
### Will Smith Standardize Dollars
```{r,mychunk-standarize-dollars-will, fig.cap= c("Will Smith scatterplot of Adjusted Millions: IMDB(2020)")}
# https://raw.githubusercontent.com/MonteShaffer/humanVerseWSU
#master/humanVerseWSU/R/functions-inflation.R

humanVerseWSU::loadInflationData();
str(will$movies.50);
will$movies.50 = standardizeDollarsInDataFrame(will$movies.50, 2000, "millions", "year", "millionsAdj");
str(will$movies.50);
plot(will$movies.50$millions,will$movies.50$millionsAdj);

# total millions earned adjusted
total.millions.adjusted_w = will$movies.50$millionsAdj;
will.millions.adjusted = 0;
for (million in total.millions.adjusted_w){
  will.millions.adjusted = will.millions.adjusted + total.millions.adjusted_w[i];
}
will.millions.adjusted;

```

### Denzel Washington Standardize Dollars
```{r,mychunk-standarize-dollars-denzel, fig.cap= c("Denzel Washington scatterplot of Adjusted Millions: IMDB(2020)")}
# https://raw.githubusercontent.com/MonteShaffer/humanVerseWSU
#master/humanVerseWSU/R/functions-inflation.R

humanVerseWSU::loadInflationData();
str(denzel$movies.50);
denzel$movies.50 = standardizeDollarsInDataFrame(denzel$movies.50, 2000, "millions", "year", "millionsAdj");
str(denzel$movies.50);
plot(denzel$movies.50$millions,denzel$movies.50$millionsAdj);

# total millions earned adjusted
total.millions.adjusted_d = denzel$movies.50$millionsAdj;
denzel.millions.adjusted = 0;
for (million in total.millions.adjusted_d){
  denzel.millions.adjusted = denzel.millions.adjusted + total.millions.adjusted_d[i];
}
denzel.millions.adjusted;
```
The observation about the earned money over the years with adjusted money the result is quite clear. Will Smith is winning this close-up analysis. He earned 10646.75 million dollars so far while Denzel Washington did earn only 5404.971 million dollars.


## Total Votes  (Divide by 1,000,000)
### Votes for Will Smith
```{r,mychunk-total-votes-will, fig.cap= c("Will Smith boxplot of Votes: IMDB(2020)")}
boxplot(will$movies$votes);
widx =  which.max(will$movies$votes);
will$movies[widx,];
total.votes.w = will$movies$votes
will.votes = 0;
for (vote in total.votes.w){
  will.votes = will.votes + total.votes.w[i];
}
will.votel.p = will.votes/1000000;
will.votel.p
```

### Votes for Denzel Washington
```{r,mychunk-total-votes-will, , fig.cap= c("Denzel Washington boxplot of Votes: IMDB(2020)")}
boxplot(denzel$movies$votes);
widx =  which.max(denzel$movies$votes);
denzel$movies[widx,];
total.votes.d = denzel$movies$votes
denzel.votes = 0;
for (vote in total.votes.d){
  denzel.votes = denzel.votes + total.votes.d[i];
}
denzel.votes.p = denzel.votes/1000000;
denzel.votes.p;
```
A more customer and viewer based comparison is when looking at votes. There all votes combined are divided by 1 million to clearly see the difference: Will Smith got 33.7651 votes and Denzel Washington did get 19.20505, which shows that Will is more popular among the audience as well.


## Average Ratings
### Average Ratings Will Smith
```{r,mychunk-total-ratings-will, , fig.cap= c("Will Smith boxplot of Ratings: IMDB(2020)")}
boxplot(will$movies$ratings);
widx =  which.max(will$movies$ratings);
will$movies[widx,];
total.ratings.w = will$movies$ratings
will.ratings = 0;
for (rating in total.ratings.w){
  will.ratings = will.ratings + total.ratings.w[i];
}
will.ratings;
summary(will$movies.$ratings);
```

### Average Ratings Denzel Washington
```{r,mychunk-total-ratings-denzel, fig.cap= c("Denzel Washington boxplot of Ratings: IMDB(2020)")}
boxplot(denzel$movies$ratings);
widx =  which.max(denzel$movies$ratings);
denzel$movies[widx,];
total.ratings.d = denzel$movies$ratings
denzel.ratings = 0;
for (rating in total.ratings.d){
  denzel.ratings = denzel.ratings + total.ratings.d[i];
}
denzel.ratings;
summary(denzel$movies.$ratings);
```
The result of the comparison on the average ratings is very interestin and close. Where all other categories have been won by Will Smith, this time Denzel is having a better rating average. 


## Who has the longest total and average Movie length in Minutes?
### Lenght of Movies Will Smith
```{r,mychunk-total-movie-length-will, fig.cap= c("Will Smith boxplot of Minutes: IMDB(2020)")}
boxplot(will$movies$minutes);
widx =  which.max(will$movies$minutes);
will$movies[widx,];
total.minutes.w = will$movies$minutes
will.minutes = 0;
for (minute in total.minutes.w){
  will.minutes = will.minutes + total.minutes.w[i];
}
will.minutes;
summary(will$movies.$minutes);
```

### Lenght of Movies Denzel Washington
```{r,mychunk-total-movie-length-denzel, fig.cap= c("Denzel Washington boxplot of Minutes: IMDB(2020)")}
boxplot(denzel$movies$minutes);
widx =  which.max(denzel$movies$minutes);
denzel$movies[widx,];
total.minutes.d = denzel$movies$minutes
denzel.minutes = 0;
for (minute in total.minutes.d){
  denzel.minutes = denzel.minutes + total.minutes.d[i];
}
denzel.minutes;
summary(denzel$movies.$minutes);
```
The average movie length seems to be longer in movies where Denzel Washington has been a part of. The total length of the Denzel movies is 7850 and of Will movies it is 5050 minutes. Looks like Denzel is playing in longer movies than Will does.


## Metacritic (NA values)
### Metacritic of Will Smith Movies
```{r,mychunk-metacritic-will, fig.cap= c("Will Smith boxplot of Metacritic: IMDB(2020)")}
boxplot(will$movies$metacritic);
widx =  which.max(will$movies$metacritic);
will$movies[widx,];
total.metacritic.w = will$movies$metacritic
will.metacritic = 0;
for (meta in total.metacritic.w){
  will.metacritic = will.metacritic + total.metacritic.w[i];
}
will.metacritic;
summary(will$movies.$metacritic);
```

### Metacritic of Denzel Washington Movies
```{r,mychunk-metacritic-denzel, fig.cap= c("Denzel Washington boxplot of Metacritic: IMDB(2020)")}
boxplot(denzel$movies$metacritic);
widx =  which.max(denzel$movies$metacritic);
denzel$movies[widx,];
total.metacritic.d = denzel$movies$metacritic
denzel.metacritic = 0;
for (meta in total.metacritic.d){
  denzel.metacritic = denzel.metacritic + total.metacritic.d[i];
}
denzel.metacritic;
summary(denzel$movies.$metacritic);
```
This last part of the analysis looks at the metacritic. Here it looks like Denzel Washington has larger metacritics than Will Smith. 

As I stated before, this question of who of this two actors is a better actor cannot be determined with statistical analysis. There can be made overall trends but it is a very subjective  answer. Everybody has a different opinion and viewpoint.
